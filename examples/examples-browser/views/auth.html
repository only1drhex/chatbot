<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face Comparison</title>
  <script src="https://cdn.jsdelivr.net/npm/face-api.js/dist/face-api.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.11.5/gsap.min.js"></script>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <style>
    body {
      background: linear-gradient(to right, #4facfe 0%, #00f2fe 100%);
    }
    video {
      width: 100%;
      max-width: 400px;
      border-radius: 8px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2);
    }
    img {
      display: none; /* Hidden for background loading */
    }
    .status, .result {
      opacity: 0;
      transform: translateY(20px);
    }
  </style>
</head>
<body class="flex flex-col items-center justify-center p-4">
  <h1 class="text-4xl font-bold mb-6 text-white">Face Comparison</h1>
  
  <div class="flex mb-6">
    <video id="video" autoplay></video>
  </div>
  
  <h2 id="status" class="text-xl text-white mb-2"></h2>
  <h2 id="result" class="text-xl text-white"></h2>
  
  <h3 id="previous-result" class="text-lg text-white mt-4"></h3>

  <script>
    const MODEL_URLS = {
      Mobilenetv1Model: "https://raw.githubusercontent.com/ml5js/ml5-data-and-models/main/models/faceapi/ssd_mobilenetv1_model-weights_manifest.json",
      FaceLandmarkModel: "https://raw.githubusercontent.com/ml5js/ml5-data-and-models/main/models/faceapi/face_landmark_68_model-weights_manifest.json",
      FaceRecognitionModel: "https://raw.githubusercontent.com/ml5js/ml5-data-and-models/main/models/faceapi/face_recognition_model-weights_manifest.json",
    };

    const REFERENCE_IMAGE_URL = './img.jpg';

    const faceDetectionFrequency = localStorage.getItem('faceDetectionFrequency') || 2000;

    async function loadModels() {
      if (localStorage.getItem('modelsLoaded') === 'true') {
        console.log('Models already loaded');
        return;
      }

      await Promise.all([
        faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URLS.Mobilenetv1Model),
        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URLS.FaceLandmarkModel),
        faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URLS.FaceRecognitionModel),
      ]);
      console.log('Models loaded');
      localStorage.setItem('modelsLoaded', 'true');
    }

    async function startVideo() {
      const video = document.getElementById('video');
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;

      setInterval(() => detectFace(video), faceDetectionFrequency);
    }

    async function loadReferenceImage() {
      const referenceImage = new Image();
      referenceImage.src = REFERENCE_IMAGE_URL;
      await referenceImage.decode();
      return referenceImage;
    }

    async function detectFace(video) {
      // Load the face descriptor from local storage if available
      const savedDescriptorString = localStorage.getItem('authenticatedFaceDescriptor');
      if (savedDescriptorString) {
        const savedDescriptor = new Float32Array(JSON.parse(savedDescriptorString));
        
        const faceDescriptor2 = await faceapi.detectSingleFace(video).withFaceLandmarks().withFaceDescriptor();
        if (!faceDescriptor2) {
          updateStatus('No live face detected');
          return;
        }

        const faceMatcher = new faceapi.FaceMatcher(savedDescriptor);
        const match = faceMatcher.findBestMatch(faceDescriptor2.descriptor);
        const similarityScore = match._distance;

        if (similarityScore < 0.6) {
          updateStatus('Face already authenticated');
          displayResult('This face was already authenticated.');
          return;
        }
      }

      // Otherwise, proceed to compare with the reference image
      const referenceImage = await loadReferenceImage();
      const faceDescriptor1 = await faceapi.detectSingleFace(referenceImage).withFaceLandmarks().withFaceDescriptor();

      if (!faceDescriptor1) {
        updateStatus('No reference face detected');
        return;
      }

      updateStatus('Reference face detected');

      const faceDescriptor2 = await faceapi.detectSingleFace(video).withFaceLandmarks().withFaceDescriptor();
      if (!faceDescriptor2) {
        updateStatus('No live face detected');
        return;
      }

      updateStatus('Live face detected, capturing...');

      const faceMatcher = new faceapi.FaceMatcher(faceDescriptor1);
      updateStatus('Comparing faces...');

      const match = faceMatcher.findBestMatch(faceDescriptor2.descriptor);
      const similarityScore = match._distance;

      let resultMessage = '';
      if (similarityScore < 0.6) {
        resultMessage = "These faces look very similar!";

        // Save the authenticated face descriptor to local storage
        localStorage.setItem('authenticatedFaceDescriptor', JSON.stringify(Array.from(faceDescriptor2.descriptor)));
      } else if (similarityScore < 0.8) {
        resultMessage = "These faces are somewhat similar.";
      } else {
        resultMessage = "These faces do not look similar.";
      }

      updateStatus('Comparison done');
      displayResult(resultMessage);

      // Save the comparison result to local storage
      localStorage.setItem('lastComparisonResult', resultMessage);
    }

    function updateStatus(message) {
      const statusElement = document.getElementById('status');
      statusElement.innerText = message;
      gsap.to(statusElement, { duration: 0.5, opacity: 1, translateY: 0, ease: "power3.out" });
    }

    function displayResult(resultMessage) {
      const resultElement = document.getElementById('result');
      resultElement.innerText = resultMessage;
      gsap.to(resultElement, { duration: 1, opacity: 1, scale: 1.2, ease: "bounce.out" });
      gsap.to(resultElement, { duration: 1, delay: 1, opacity: 1, scale: 1, ease: "bounce.out" });
    }

    function displayPreviousResult() {
      const previousResult = localStorage.getItem('lastComparisonResult');
      if (previousResult) {
        const previousResultElement = document.getElementById('previous-result');
        previousResultElement.innerText = Previous comparison result: ${previousResult};
        gsap.to(previousResultElement, { duration: 1, opacity: 1, translateY: 0, ease: "power3.out" });
      }
    }

    loadModels().then(() => {
      startVideo();
      displayPreviousResult();
    });
  </script>
</body>
</html>